{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8cb51d4",
   "metadata": {},
   "source": [
    "# TwelveLabs / Amazon OpenSearch Demonstration\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "1. Establish a free TwelveLabs account and obtain an [API Key](https://playground.twelvelabs.io/dashboard/api-keys).\n",
    "2. Create an [Amazon OpenSearch Serverless Collection](https://us-east-1.console.aws.amazon.com/aos/home?region=us-east-1#opensearch/collections) and note the OpenSearch endpoint, ignoring the `https://` prefix.\n",
    "\n",
    "### Sample Video Content\n",
    "\n",
    "Download free videos from any number of sites, including [Pexels](https://www.pexels.com/videos/). Videos must meet the TwelveLabs [requirements](https://docs.twelvelabs.io/docs/get-started/quickstart/create-embeddings#prerequisites):\n",
    "\n",
    "- Video resolution: Must be at least 360x360 and must not exceed 3840x2160.\n",
    "- Aspect ratio: Must be one of 1:1, 4:3, 4:5, 5:4, 16:9, or 9:16.\n",
    "- I suggest starting with Pexel's small SD size, 640 x 360 pixels, (16:9) format videos for speed and cost.\n",
    "- I used 25 videos as a minimum to obtain reasonable search results.\n",
    "\n",
    "### Workflow Diagram\n",
    "\n",
    "![Architecture](./twelve_labs_bedrock.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860e9601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages using pip\n",
    "%pip install pip -Uq\n",
    "%pip install twelvelabs boto3 opensearch-py -Uq\n",
    "%pip install matplotlib Pillow scikit-learn plotly nbformat -Uq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6e8f20",
   "metadata": {},
   "source": [
    "#### Restart Kernel\n",
    "\n",
    "If first time installing, restart your Jupyter Notebook's kernel before continuing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0608c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that the Twelve Labs package is installed\n",
    "%pip show twelvelabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8963b493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import json\n",
    "import os\n",
    "\n",
    "from twelvelabs import TwelveLabs\n",
    "from twelvelabs.models import Video\n",
    "from twelvelabs.exceptions import NotFoundError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ec95d4",
   "metadata": {},
   "source": [
    "## TwelveLabs API Key and AWS Credentials\n",
    "\n",
    "Set TwelveLabs API Key and AWS Credentials as environment variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1a57c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Make sure to update these variables before running the code ***\n",
    "%env AWS_REGION=<Your AWS Region>\n",
    "%env AWS_ACCESS_KEY_ID=<Your AWS Access Key ID>\n",
    "%env AWS_SECRET_ACCESS_KEY=<Your AWS Secret Access Key>\n",
    "%env AWS_SESSION_TOKEN=<Your AWS Session Token>\n",
    "%env TL_API_KEY=<Your TL API Key>\n",
    "%env OPENSEARCH_ENDPOINT=<Your OpenSearch Endpoint>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033d1234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the API key for TwelveLabs from environment variable\n",
    "TL_API_KEY = os.getenv(\"TL_API_KEY\").replace(\"'\", \"\").replace('\"', \"\")\n",
    "\n",
    "# Set the TwelveLabs Index ID\n",
    "TL_INDEX_NAME = \"pexels_sample_index\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41ae3bb",
   "metadata": {},
   "source": [
    "## Create TwelveLabs Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc0c776",
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_client = TwelveLabs(api_key=TL_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1e12ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(index_name: str) -> str:\n",
    "    \"\"\"Create a new index for embeddings if it doesn't already exist.\n",
    "\n",
    "    Args:\n",
    "        index_name (str): The name of the index to create.\n",
    "\n",
    "    Returns:\n",
    "        str: The ID of the created index.\n",
    "    \"\"\"\n",
    "    # Check if the index already exists\n",
    "    index_list = tl_client.index.list(\n",
    "        name=index_name,\n",
    "        sort_option=\"asc\",\n",
    "        page_limit=1,\n",
    "    )\n",
    "\n",
    "    # If the index exists, return its ID\n",
    "    if index_list:\n",
    "        for index in index_list:\n",
    "            print(f\"Index '{index.name}' already exists.\")\n",
    "            return index.id\n",
    "\n",
    "    # If the index does not exist, create a new one\n",
    "    print(f\"Creating index '{index_name}'...\")\n",
    "    models = [\n",
    "        {\"name\": \"marengo2.7\", \"options\": [\"visual\", \"audio\"]},\n",
    "        {\"name\": \"pegasus1.2\", \"options\": [\"visual\", \"audio\"]},\n",
    "    ]\n",
    "\n",
    "    created_index = tl_client.index.create(\n",
    "        name=index_name, models=models, addons=[\"thumbnail\"]\n",
    "    )\n",
    "\n",
    "    return created_index.id\n",
    "\n",
    "\n",
    "tl_index_id = create_index(TL_INDEX_NAME)\n",
    "print(f\"New index ID: {tl_index_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afda97ca",
   "metadata": {},
   "source": [
    "## Upload Videos to Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6130f2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_video(tl_index_id: str, video_path: str) -> None:\n",
    "    \"\"\"Upload a video to the TwelveLabs index.\n",
    "\n",
    "    Args:\n",
    "        tl_index_id (str): The ID of the TwelveLabs index.\n",
    "        video_path (str): The path to the video file to upload.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        task = tl_client.task.create(index_id=tl_index_id, file=video_path)\n",
    "        print(f\"Task id={task.id}\")\n",
    "        print(f\"Video '{video_path}' uploaded successfully!\")\n",
    "    except Exception as ex:\n",
    "        print(f\"Failed to upload video '{video_path}': {ex}\")\n",
    "\n",
    "\n",
    "video_directory = \"videos/pexels\"\n",
    "if not os.path.exists(video_directory):\n",
    "    print(f\"Video directory '{video_directory}' does not exist. Creating it.\")\n",
    "    os.makedirs(video_directory)\n",
    "\n",
    "for video in os.listdir(video_directory):\n",
    "    if video.endswith(\".mp4\"):\n",
    "        video_path = os.path.join(video_directory, video)\n",
    "        upload_video(tl_index_id, video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f9c48b",
   "metadata": {},
   "source": [
    "## Retrieve Embeddings and Analyses from Index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be0c737",
   "metadata": {},
   "source": [
    "### Bulk Retrieve Embeddings from Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6850454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings_to_json(video: Video, output_path: str) -> None:\n",
    "    \"\"\"Save the embedding task details to a JSON file if it doesn't already exist.\n",
    "\n",
    "    Args:\n",
    "        video (Video): The video object containing embedding details.\n",
    "        output_path (str): The path where the JSON file will be saved.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Serialize the video object to a dictionary\n",
    "    video_data = video.model_dump_json()\n",
    "    video_data = json.loads(video_data)\n",
    "    video_data[\"video_id\"] = video.id\n",
    "\n",
    "    # Determine the filename using the input filename from the task metadata\n",
    "    input_filename = video_data[\"video_id\"]\n",
    "    output_filename = f\"{output_path}/{input_filename}_embeddings.json\"\n",
    "    if os.path.exists(output_filename):\n",
    "        print(f\"Embeddings already exist for video ID {video.id}. Skipping...\")\n",
    "        return\n",
    "\n",
    "    # Write the dictionary to a JSON file\n",
    "    with open(output_filename, \"w\") as json_file:\n",
    "        json.dump(video_data, json_file, indent=4)\n",
    "    print(f\"Embeddings saved to {output_filename}\")\n",
    "\n",
    "\n",
    "def get_videos_from_index(index_id: str, page_limit: int = 25) -> list:\n",
    "    \"\"\"Retrieve video IDs from the specified index.\n",
    "\n",
    "    Args:\n",
    "        index_id (str): The ID of the index to query.\n",
    "        page_limit (int): The maximum number of results to return.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of video IDs retrieved from the index.\n",
    "    \"\"\"\n",
    "    result = tl_client.search.query(\n",
    "        index_id=index_id,\n",
    "        query_text=\"*\",\n",
    "        options=[\"visual\"],\n",
    "        page_limit=page_limit,\n",
    "    )\n",
    "\n",
    "    print(f\"Total count of videos in index {index_id}: {result.pool.total_count}\")\n",
    "    if result.pool.total_count == 0:\n",
    "        raise NotFoundError(f\"No videos found in index {index_id}\")\n",
    "    print(result)\n",
    "    video_ids = [item.video_id for item in result.data]\n",
    "    return video_ids\n",
    "\n",
    "\n",
    "# Retrieve the video IDs from the index\n",
    "video_ids = get_videos_from_index(tl_index_id)\n",
    "\n",
    "# Retrieve the video embeddings from the index and save to JSON\n",
    "for video_id in video_ids:\n",
    "    video = tl_client.index.video.retrieve(\n",
    "        index_id=tl_index_id, id=video_id, embedding_option=[\"visual-text\"]\n",
    "    )\n",
    "\n",
    "    output_directory = \"output/pexels\"\n",
    "    if not os.path.exists(output_directory):\n",
    "        print(f\"Output directory '{output_directory}' does not exist. Creating it.\")\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    print(f\"Processing video ID: {video.id}\")\n",
    "    save_embeddings_to_json(video, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997ee245",
   "metadata": {},
   "source": [
    "### Bulk Create Analyses from Videos in Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665f859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_video(index_id: str, video_id: str, output_path: str) -> None:\n",
    "    \"\"\"Summarize a video and save the analysis to a JSON file if it doesn't already exist.\n",
    "\n",
    "    Args:\n",
    "        index_id (str): The ID of the index where the video is stored.\n",
    "        video_id (str): The ID of the video to summarize.\n",
    "        output_path (str): The path where the JSON file will be saved.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Check if the analysis already exists\n",
    "    filename = f\"{output_path}/{video_id}_analysis.json\"\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"Analysis already exists for video ID {video_id}. Skipping...\")\n",
    "        return\n",
    "    print(f\"Analyzing video ID: {video_id}\")\n",
    "\n",
    "    # Get the video summary\n",
    "    res_summary = tl_client.summarize(\n",
    "        video_id=video_id,\n",
    "        prompt=\"Summarize the video in a concise manner.\",\n",
    "        temperature=0.4,\n",
    "        type=\"summary\",\n",
    "    )\n",
    "\n",
    "    # Get the chapters of the video\n",
    "    res_chapters = tl_client.summarize(\n",
    "        video_id=video_id,\n",
    "        prompt=\"List the chapters of the video.\",\n",
    "        temperature=0.4,\n",
    "        type=\"chapter\",\n",
    "    )\n",
    "\n",
    "    # Get the highlights of the video\n",
    "    res_highlights = tl_client.summarize(\n",
    "        video_id=video_id,\n",
    "        prompt=\"List the highlights of the video.\",\n",
    "        temperature=0.4,\n",
    "        type=\"highlight\",\n",
    "    )\n",
    "\n",
    "    # Get open-ended text analysis of the video\n",
    "    res_analyze = tl_client.analyze(\n",
    "        video_id=video_id,\n",
    "        prompt=\"Describe what is happening in the video.\",\n",
    "        temperature=0.4,\n",
    "    )\n",
    "\n",
    "    # Get the gist of the video\n",
    "    res_gist = tl_client.gist(video_id=video_id, types=[\"title\", \"topic\", \"hashtag\"])\n",
    "\n",
    "    # Combined responses\n",
    "    analyses = {}\n",
    "\n",
    "    analyses.update(\n",
    "        {\n",
    "            \"gist\": res_gist.model_dump(),\n",
    "            \"video_id\": video_id,\n",
    "            \"index_id\": index_id,\n",
    "            \"summary\": res_summary.summary,\n",
    "            \"analysis\": res_analyze.data,\n",
    "            \"chapters\": res_chapters.chapters.model_dump(),\n",
    "            \"highlights\": res_highlights.highlights.model_dump(),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # save to file\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(json.dumps(analyses))\n",
    "\n",
    "\n",
    "# Retrieve the video IDs from the index\n",
    "video_ids = get_videos_from_index(tl_index_id)\n",
    "\n",
    "# Retrieve the video embeddings from the index and save to JSON\n",
    "for video_id in video_ids:\n",
    "    video = tl_client.index.video.retrieve(\n",
    "        index_id=tl_index_id, id=video_id, embedding_option=[\"visual-text\"]\n",
    "    )\n",
    "    summarize_video(tl_index_id, video.id, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110b3c35",
   "metadata": {},
   "source": [
    "### Merge Embeddings and Analyses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff68d2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_video_ids(output_path: str) -> list:\n",
    "    \"\"\"Extract video IDs from analysis filenames in the specified directory.\n",
    "\n",
    "    Args:\n",
    "        output_path (str): Directory containing the analysis JSON files\n",
    "\n",
    "    Returns:\n",
    "        list: List of extracted video IDs\n",
    "    \"\"\"\n",
    "    video_ids = []\n",
    "\n",
    "    # Check if the output directory exists\n",
    "    if not os.path.exists(output_path):\n",
    "        print(f\"Directory {output_path} doesn't exist\")\n",
    "        return video_ids\n",
    "\n",
    "    for filename in os.listdir(output_path):\n",
    "        # Check if it's an analysis file\n",
    "        if filename.endswith(\"_analysis.json\"):\n",
    "            # Extract the ID part from the filename\n",
    "            # The ID is everything before \"_analysis.json\"\n",
    "            video_id = filename.split(\"_analysis.json\")[0]\n",
    "            video_ids.append(video_id)\n",
    "\n",
    "    return video_ids\n",
    "\n",
    "\n",
    "# Extract video IDs from the analysis files\n",
    "video_ids = extract_video_ids(output_directory)\n",
    "print(f\"Found {len(video_ids)} video IDs: {video_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a658e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_segments_to_documents(\n",
    "    output_path: str, document_path: str, video_ids: list\n",
    ") -> None:\n",
    "    \"\"\"Combine embeddings and analyses into single documents and save them to a local directory.\n",
    "\n",
    "    Args:\n",
    "        output_path (str): Directory containing the analysis and embeddings JSON files\n",
    "        document_path (str): Directory to save the combined document files\n",
    "        video_ids (list): List of video IDs to process\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for video_id in video_ids:\n",
    "        # Open corresponding analyses and embeddings documents and combined\n",
    "        with open(f\"{output_path}/{video_id}_embeddings.json\", \"r\") as f:\n",
    "            embeddings = json.load(f)\n",
    "\n",
    "        with open(f\"{output_path}/{video_id}_analysis.json\", \"r\") as f:\n",
    "            analyses = json.load(f)\n",
    "\n",
    "        # Combine the two documents\n",
    "        document = {}\n",
    "        document.update(analyses)\n",
    "        document.update(embeddings)\n",
    "\n",
    "        # Remove unneeded keys\n",
    "        document[\"gist\"].pop(\"id\", None)\n",
    "        document[\"gist\"].pop(\"usage\", None)\n",
    "\n",
    "        # Segments of video\n",
    "        segments = document[\"embedding\"][\"video_embedding\"][\"segments\"]\n",
    "\n",
    "        # Write documents to local directory for each segment\n",
    "        filename = f\"{document_path}/{document['video_id']}_document.json\"\n",
    "        document.pop(\"embedding\", None)\n",
    "        document[\"segments\"] = segments\n",
    "        for segment in document[\"segments\"]:\n",
    "            segment[\"segment_embedding\"] = segment[\"embeddings_float\"].copy()\n",
    "            segment.pop(\"embeddings_float\", None)\n",
    "\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(json.dumps(document, indent=4))\n",
    "\n",
    "\n",
    "document_directory = \"documents/pexels\"\n",
    "if not os.path.exists(document_directory):\n",
    "    print(f\"Document directory '{document_directory}' does not exist. Creating it.\")\n",
    "    os.makedirs(document_directory)\n",
    "\n",
    "combine_segments_to_documents(output_directory, document_directory, video_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f4b1de",
   "metadata": {},
   "source": [
    "## Amazon OpenSearch Serverless\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60b5d57",
   "metadata": {},
   "source": [
    "### Instantiate OpenSearch Client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbc4429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "from opensearchpy import (\n",
    "    AWSV4SignerAuth,\n",
    "    NotFoundError,\n",
    "    OpenSearch,\n",
    "    RequestsHttpConnection,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e78ff84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amazon OpenSearch configuration\n",
    "aws_access_key_id = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret_access_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "aws_session_token = os.getenv(\"AWS_SESSION_TOKEN\")\n",
    "\n",
    "aws_region = os.getenv(\"AWS_REGION\", \"us-east-1\")\n",
    "aoss_host = os.getenv(\"OPENSEARCH_ENDPOINT\")\n",
    "aoss_index = os.getenv(\"OPENSEARCH_INDEX\", \"video-search-nested\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331c5396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create OpenSearch client\n",
    "# https://opensearch.org/docs/latest/clients/python-low-level/#connecting-to-amazon-opensearch-serverless\n",
    "\n",
    "service = \"aoss\"\n",
    "credentials = boto3.Session(\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    aws_session_token=aws_session_token,\n",
    "    region_name=aws_region,\n",
    ").get_credentials()\n",
    "auth = AWSV4SignerAuth(credentials, aws_region, service)\n",
    "\n",
    "aoss_client = OpenSearch(\n",
    "    hosts=[{\"host\": aoss_host, \"port\": 443}],\n",
    "    http_auth=auth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    pool_maxsize=20,\n",
    ")\n",
    "\n",
    "aoss_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b041a8e4",
   "metadata": {},
   "source": [
    "### Create New OpenSearch Vector Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7a8043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new nested field search index (multiple vector fields)\n",
    "# https://docs.opensearch.org/docs/latest/vector-search/specialized-operations/nested-search-knn/\n",
    "\n",
    "try:\n",
    "    response = aoss_client.indices.delete(index=aoss_index)\n",
    "except NotFoundError as ex:\n",
    "    print(f\"Index {aoss_index} not found, skipping deletion.\")\n",
    "except Exception as ex:\n",
    "    print(f\"Error deleting index: {ex}\")\n",
    "\n",
    "index_body = {\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "            \"knn\": True,\n",
    "            \"number_of_shards\": 2,\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"segments\": {\n",
    "                \"type\": \"nested\",\n",
    "                \"properties\": {\n",
    "                    \"segment_embedding\": {\n",
    "                        \"type\": \"knn_vector\",\n",
    "                        \"dimension\": 1024,\n",
    "                        \"method\": {\n",
    "                            \"engine\": \"faiss\",\n",
    "                            \"name\": \"hnsw\",\n",
    "                            \"space_type\": \"l2\",\n",
    "                        },\n",
    "                    }\n",
    "                },\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = aoss_client.indices.create(index=aoss_index, body=index_body)\n",
    "    print(json.dumps(response, indent=4))\n",
    "except Exception as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cef4663",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = aoss_client.indices.get(index=aoss_index)\n",
    "    print(json.dumps(response, indent=4))\n",
    "except NotFoundError as ex:\n",
    "    print(f\"Index not found: {ex}\")\n",
    "except Exception as ex:\n",
    "    print(ex.error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad06a2f",
   "metadata": {},
   "source": [
    "### Bulk Index OpenSearch Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10219d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bulk indexing documents from JSON files in the local directory\n",
    "def load_and_index_documents(document_path: str) -> None:\n",
    "    \"\"\"Load documents from JSON files in the specified directory and index them in OpenSearch.\n",
    "\n",
    "    Args:\n",
    "        document_path (str): Directory containing the document JSON files\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    payload = \"\"\n",
    "    put_command = f'{{ \"create\": {{ \"_index\": \"{aoss_index}\" }} }}\\n'\n",
    "\n",
    "    for file in os.listdir(document_path):\n",
    "        if file.endswith(\"_document.json\"):\n",
    "            with open(os.path.join(document_path, file), \"r\") as f:\n",
    "                tmp = json.load(f)\n",
    "                payload += f\"{put_command}{json.dumps(tmp)}\\n\"\n",
    "\n",
    "    try:\n",
    "        response = aoss_client.bulk(\n",
    "            index=aoss_index,\n",
    "            body=payload,\n",
    "        )\n",
    "        print(json.dumps(response, indent=4))\n",
    "        row_count = int(len(payload.splitlines()) / 2)\n",
    "        return row_count\n",
    "    except Exception as ex:\n",
    "        print(f\"Error indexing documents: {ex}\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "row_count = load_and_index_documents(document_directory)\n",
    "print(f\"Total rows to index: {row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df0e5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "# Wait for indexing to complete and refresh\n",
    "response = aoss_client.count(index=aoss_index)\n",
    "while response[\"count\"] != row_count:\n",
    "    response = aoss_client.count(index=aoss_index)\n",
    "    print(f\"Current indexed documents: {response['count']}\")\n",
    "    sleep(10)\n",
    "print(f\"Indexing completed. Total indexed documents: {response['count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80085f1c",
   "metadata": {},
   "source": [
    "## Query the Amazon OpenSearch Index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ee0925",
   "metadata": {},
   "source": [
    "### Convert Query to Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd6be6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_from_query(query: str) -> list:\n",
    "    \"\"\"Convert a text query to an embedding using TwelveLabs.\n",
    "\n",
    "    Args:\n",
    "        query (str): The text query to convert.\n",
    "\n",
    "    Returns:\n",
    "        list: The embedding vector.\n",
    "    \"\"\"\n",
    "    res = tl_client.embed.create(\n",
    "        model_name=\"Marengo-retrieval-2.7\",\n",
    "        text_truncate=\"start\",\n",
    "        text=query,\n",
    "    )\n",
    "\n",
    "    if res.text_embedding is not None and res.text_embedding.segments is not None:\n",
    "        return res.text_embedding.segments[0].embeddings_float\n",
    "    else:\n",
    "        raise ValueError(\"Failed to retrieve embedding from the response.\")\n",
    "\n",
    "\n",
    "query = \"bustling street scene from a low-angle perspective\"\n",
    "query_embedding = get_embedding_from_query(query)\n",
    "print(f\"Embedding: {query_embedding[:5]}...\")  # Print first 5 elements for brevity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4601932b",
   "metadata": {},
   "source": [
    "### Semantic Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d698c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://docs.opensearch.org/docs/latest/vector-search/filter-search-knn/efficient-knn-filtering/#faiss-k-nn-filter-implementation\n",
    "\n",
    "\n",
    "def semantic_search(aoss_index: str, embedding: list) -> dict:\n",
    "    \"\"\"Query the OpenSearch index using a text embedding.\n",
    "\n",
    "    Args:\n",
    "        aoss_index (str): The ID of the Amazon OpenSearch index.\n",
    "        embedding (list): The embedding vector to use for the query.\n",
    "\n",
    "    Returns:\n",
    "        dict: The search response from OpenSearch.\n",
    "    \"\"\"\n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"nested\": {\n",
    "                \"path\": \"segments\",\n",
    "                \"query\": {\n",
    "                    \"knn\": {\n",
    "                        \"segments.segment_embedding\": {\n",
    "                            \"vector\": embedding,\n",
    "                            \"k\": 6,\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "        \"size\": 6,\n",
    "        \"_source\": {\"excludes\": [\"segments.segment_embedding\"]},\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        search_results = aoss_client.search(body=query, index=aoss_index)\n",
    "        return search_results\n",
    "    except Exception as ex:\n",
    "        print(f\"Error querying index: {ex}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "# Query the index with the embedding\n",
    "search_results_1 = semantic_search(aoss_index, query_embedding)\n",
    "\n",
    "for hit in search_results_1[\"hits\"][\"hits\"]:\n",
    "    print(f\"Video ID: {hit['_source'].get('video_id', 'N/A')}\")\n",
    "    print(f\"Title: {hit['_source'].get('title', 'N/A')}\")\n",
    "    print(f\"Score: {hit['_score']}\")\n",
    "    print(f\"Duration: {hit['_source']['system_metadata']['duration']:.2f} seconds\")\n",
    "    print(\"\\r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2598e3",
   "metadata": {},
   "source": [
    "### Semantic Search with Filters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08bf6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://docs.opensearch.org/docs/latest/vector-search/filter-search-knn/efficient-knn-filtering/#step-3-search-your-data-with-a-filter\n",
    "\n",
    "\n",
    "def semantic_search_with_filter(aoss_index: str, embedding: list) -> dict:\n",
    "    \"\"\"Query the OpenSearch index using a text embedding.\n",
    "\n",
    "    Args:\n",
    "        aoss_index (str): The ID of the Amazon OpenSearch index.\n",
    "        embedding (list): The embedding vector to use for the query.\n",
    "\n",
    "    Returns:\n",
    "        dict: The search response from OpenSearch.\n",
    "    \"\"\"\n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"nested\": {\n",
    "                \"path\": \"segments\",\n",
    "                \"query\": {\n",
    "                    \"knn\": {\n",
    "                        \"segments.segment_embedding\": {\n",
    "                            \"vector\": embedding,\n",
    "                            \"k\": 5,\n",
    "                            \"filter\": {\n",
    "                                \"bool\": {\n",
    "                                    \"must\": [\n",
    "                                        {\n",
    "                                            \"range\": {\n",
    "                                                \"system_metadata.duration\": {\n",
    "                                                    \"gte\": 20,\n",
    "                                                    \"lte\": 60,\n",
    "                                                }\n",
    "                                            }\n",
    "                                        },\n",
    "                                    ]\n",
    "                                }\n",
    "                            },\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "        \"size\": 5,\n",
    "        \"_source\": {\"excludes\": [\"segments.segment_embedding\"]},\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        search_results = aoss_client.search(body=query, index=aoss_index)\n",
    "        return search_results\n",
    "    except Exception as ex:\n",
    "        print(f\"Error querying index: {ex}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "# Query the index with the embedding\n",
    "search_results_2 = semantic_search_with_filter(aoss_index, query_embedding)\n",
    "\n",
    "for hit in search_results_2[\"hits\"][\"hits\"]:\n",
    "    print(f\"Video ID: {hit['_source'].get('video_id', 'N/A')}\")\n",
    "    print(f\"Title: {hit['_source'].get('title', 'N/A')}\")\n",
    "    print(f\"Score: {hit['_score']}\")\n",
    "    print(f\"Duration: {hit['_source']['system_metadata']['duration']:.2f} seconds\")\n",
    "    print(\"\\r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683a169",
   "metadata": {},
   "source": [
    "### Radial Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b930dc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://docs.opensearch.org/docs/latest/vector-search/specialized-operations/radial-search-knn/\n",
    "\n",
    "\n",
    "def radial_search(aoss_index: str, embedding: list) -> dict:\n",
    "    \"\"\"Query the OpenSearch index using a text embedding.\n",
    "\n",
    "    Args:\n",
    "        aoss_index (str): The ID of the Amazon OpenSearch index.\n",
    "        embedding (list): The embedding vector to use for the query.\n",
    "\n",
    "    Returns:\n",
    "        dict: The search response from OpenSearch.\n",
    "    \"\"\"\n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"nested\": {\n",
    "                \"path\": \"segments\",\n",
    "                \"query\": {\n",
    "                    \"knn\": {\n",
    "                        \"segments.segment_embedding\": {\n",
    "                            \"vector\": embedding,\n",
    "                            \"max_distance\": 2,\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "        \"size\": 6,\n",
    "        \"_source\": {\"excludes\": [\"segments.segment_embedding\"]},\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        search_results = aoss_client.search(body=query, index=aoss_index)\n",
    "        return search_results\n",
    "    except Exception as ex:\n",
    "        print(f\"Error querying index: {ex}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "# Query the index with the embedding\n",
    "search_results_3 = semantic_search(aoss_index, query_embedding)\n",
    "\n",
    "for hit in search_results_3[\"hits\"][\"hits\"]:\n",
    "    print(f\"Video ID: {hit['_source'].get('video_id', 'N/A')}\")\n",
    "    print(f\"Title: {hit['_source'].get('title', 'N/A')}\")\n",
    "    print(f\"Score: {hit['_score']}\")\n",
    "    print(f\"Duration: {hit['_source']['system_metadata']['duration']:.2f} seconds\")\n",
    "    print(\"\\r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b2103e",
   "metadata": {},
   "source": [
    "### Displaying Previews of Search Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7934d032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from urllib import request\n",
    "import io\n",
    "\n",
    "\n",
    "def load_image_from_url(url):\n",
    "    \"\"\"Load an image from a URL.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the image to load.\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image.Image: The loaded image.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with request.urlopen(url) as response:\n",
    "            image_data = response.read()\n",
    "            image = Image.open(io.BytesIO(image_data))\n",
    "            return image\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading video thumbnail from URL: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "index = 1\n",
    "rows = 3\n",
    "columns = 3\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "\n",
    "for hit in search_results_1[\"hits\"][\"hits\"]:\n",
    "    fig.set_dpi(300)\n",
    "    fig.add_subplot(rows, columns, index)\n",
    "    image_url = hit[\"_source\"][\"hls\"][\"thumbnail_urls\"][0]\n",
    "    image = load_image_from_url(image_url)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image)\n",
    "    plt.title(\n",
    "        f'Video: {hit[\"_source\"][\"system_metadata\"][\"filename\"]}\\nScore: {hit[\"_score\"]}',\n",
    "        fontdict=dict(family=\"Arial\", size=8),\n",
    "        color=\"black\",\n",
    "    )\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250733bf",
   "metadata": {},
   "source": [
    "### 2D/3D Visualizations Using PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4eba70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search_pca(aoss_index: str, embedding: list) -> dict:\n",
    "    \"\"\"Query the OpenSearch index using a text embedding.\n",
    "\n",
    "    Args:\n",
    "        aoss_index (str): The ID of the Amazon OpenSearch index.\n",
    "        embedding (list): The embedding vector to use for the query.\n",
    "\n",
    "    Returns:\n",
    "        dict: The search response from OpenSearch.\n",
    "    \"\"\"\n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"nested\": {\n",
    "                \"path\": \"segments\",\n",
    "                \"query\": {\n",
    "                    \"knn\": {\n",
    "                        \"segments.segment_embedding\": {\n",
    "                            \"vector\": embedding,\n",
    "                            \"k\": 9,\n",
    "                        },\n",
    "                    },\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "        \"size\": 9,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        search_results = aoss_client.search(body=query, index=aoss_index)\n",
    "        return search_results\n",
    "    except Exception as ex:\n",
    "        print(f\"Error querying index: {ex}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "# Query the index with the embedding\n",
    "search_results_4 = semantic_search_pca(aoss_index, query_embedding)\n",
    "\n",
    "embeddings = []\n",
    "video_names = []\n",
    "\n",
    "for hit in search_results_4[\"hits\"][\"hits\"]:\n",
    "    embeddings.append(hit[\"_source\"][\"segments\"][0][\"segment_embedding\"])\n",
    "    video_names.append(hit[\"_source\"][\"system_metadata\"][\"filename\"])\n",
    "embeddings.append(query_embedding)\n",
    "video_names.append(\"User query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9e165b",
   "metadata": {},
   "source": [
    "#### 2D Visualization Using PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46de7dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduce dimensions from 1,024 to 2 using PCA for visualization\n",
    "pca = PCA(n_components=2)\n",
    "vis_dims_2d = pca.fit_transform(embeddings)\n",
    "print(f\"Reduced dimensions shape (2d): {vis_dims_2d.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8061da9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import numpy as np\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Search results\n",
    "for i, video_name in enumerate(video_names[0:-1]):\n",
    "    x = np.array([vis_dims_2d[i, 0]])\n",
    "    y = np.array([vis_dims_2d[i, 1]])\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            mode=\"markers\",\n",
    "            marker=dict(\n",
    "                size=15,\n",
    "                colorscale=\"Viridis\",\n",
    "                opacity=1.0,\n",
    "                symbol=\"circle\",\n",
    "            ),\n",
    "            name=video_names[i],\n",
    "            # text=video_names[i],\n",
    "            # textposition=\"bottom left\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "# User query\n",
    "x = np.array([vis_dims_2d[-1, 0]])\n",
    "y = np.array([vis_dims_2d[-1, 1]])\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        mode=\"text+markers\",\n",
    "        marker=dict(\n",
    "            size=15,\n",
    "            color=\"black\",\n",
    "            colorscale=\"Viridis\",\n",
    "            opacity=1.0,\n",
    "            symbol=\"square\",\n",
    "        ),\n",
    "        name=video_names[-1],\n",
    "        text=video_names[-1],\n",
    "        textposition=\"bottom left\",\n",
    "        showlegend=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=True,\n",
    "    font=dict(size=12, color=\"black\", family=\"Arial, sans-serif\"),\n",
    "    title=\"2D Scatter Plot of Search Results using PCA\",\n",
    "    margin=dict(l=30, r=30, b=30, t=60, pad=10),\n",
    "    xaxis=dict(title=\"x\"),\n",
    "    yaxis=dict(title=\"y\"),\n",
    "    legend=dict(title=\"   Search Results\"),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5160c095",
   "metadata": {},
   "source": [
    "#### 3D Visualization Using PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179e59a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensions from 1,024 to 3 using PCA for visualization\n",
    "pca = PCA(n_components=3)\n",
    "vis_dims_3d = pca.fit_transform(embeddings)\n",
    "\n",
    "print(f\"Reduced dimensions shape (3d): {vis_dims_3d.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf7066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "# Results\n",
    "for i, video_name in enumerate(video_names[0:-1]):\n",
    "    x = np.array([vis_dims_3d[i, 0]])\n",
    "    y = np.array([vis_dims_3d[i, 1]])\n",
    "    z = np.array([vis_dims_3d[i, 2]])\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            z=z,\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=7, colorscale=\"Viridis\", opacity=1.0, symbol=\"circle\"),\n",
    "            name=video_name,\n",
    "            text=video_name,\n",
    "            textposition=\"top center\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "# User query\n",
    "x = np.array([vis_dims_3d[-1, 0]])\n",
    "y = np.array([vis_dims_3d[-1, 1]])\n",
    "z = np.array([vis_dims_3d[-1, 2]])\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter3d(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        z=z,\n",
    "        mode=\"markers\",\n",
    "        marker=dict(\n",
    "            size=7, color=\"black\", colorscale=\"Viridis\", opacity=1.0, symbol=\"square\"\n",
    "        ),\n",
    "        name=\"video_names[-1]\",\n",
    "        text=video_names[-1],\n",
    "        textposition=\"bottom left\",\n",
    "        showlegend=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=True,\n",
    "    font=dict(size=12, color=\"black\", family=\"Arial, sans-serif\"),\n",
    "    title=\"3D Scatter Plot of Search Results using PCA\",\n",
    "    margin=dict(l=30, r=30, b=20, t=50, pad=10),\n",
    "    scene=dict(\n",
    "        xaxis=dict(title=\"z\"),\n",
    "        yaxis=dict(title=\"x\"),\n",
    "        zaxis=dict(title=\"y\"),\n",
    "    ),\n",
    "    legend=dict(\n",
    "        title=\"   Search Results\",\n",
    "    ),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdab5f30",
   "metadata": {},
   "source": [
    "#### Animate the 3D Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95902df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "# Results\n",
    "for i, video_name in enumerate(video_names[0:-1]):\n",
    "    x = np.array([vis_dims_3d[i, 0]])\n",
    "    y = np.array([vis_dims_3d[i, 1]])\n",
    "    z = np.array([vis_dims_3d[i, 2]])\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            z=z,\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=7, colorscale=\"Viridis\", opacity=1.0, symbol=\"circle\"),\n",
    "            name=video_name,\n",
    "            text=video_name,\n",
    "            textposition=\"top center\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "# User query\n",
    "x = np.array([vis_dims_3d[-1, 0]])\n",
    "y = np.array([vis_dims_3d[-1, 1]])\n",
    "z = np.array([vis_dims_3d[-1, 2]])\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter3d(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        z=z,\n",
    "        mode=\"markers\",\n",
    "        marker=dict(\n",
    "            size=7, color=\"black\", colorscale=\"Viridis\", opacity=1.0, symbol=\"square\"\n",
    "        ),\n",
    "        name=video_names[-1],\n",
    "        text=video_names[-1],\n",
    "        textposition=\"top center\",\n",
    "        showlegend=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "x_eye = -1.25\n",
    "y_eye = 1.5\n",
    "z_eye = 0.5\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=True,\n",
    "    font=dict(size=12, color=\"black\", family=\"Arial, sans-serif\"),\n",
    "    title=\"3D Scatter Plot of Search Results using PCA\",\n",
    "    margin=dict(l=30, r=30, b=30, t=40, pad=10),\n",
    "    scene=dict(\n",
    "        xaxis=dict(title=\"z\"),\n",
    "        yaxis=dict(title=\"x\"),\n",
    "        zaxis=dict(title=\"y\"),\n",
    "    ),\n",
    "    scene_camera_eye=dict(x=x_eye, y=y_eye, z=z_eye),\n",
    "    updatemenus=[\n",
    "        dict(\n",
    "            type=\"buttons\",\n",
    "            showactive=True,\n",
    "            y=0.9,\n",
    "            x=0.9,\n",
    "            xanchor=\"left\",\n",
    "            yanchor=\"bottom\",\n",
    "            pad=dict(t=45, r=10),\n",
    "            buttons=[\n",
    "                dict(\n",
    "                    label=\"Play\",\n",
    "                    method=\"animate\",\n",
    "                    args=[\n",
    "                        None,\n",
    "                        dict(\n",
    "                            frame=dict(duration=15, redraw=True),\n",
    "                            transition=dict(duration=1),\n",
    "                            fromcurrent=True,\n",
    "                            mode=\"immediate\",\n",
    "                        ),\n",
    "                    ],\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "    ],\n",
    "    legend=dict(\n",
    "        title=\"   Search Results\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "def rotate_z(x, y, z, theta):\n",
    "    w = x + 1j * y\n",
    "    return np.real(np.exp(1j * theta) * w), np.imag(np.exp(1j * theta) * w), z\n",
    "\n",
    "\n",
    "frames = []\n",
    "for t in np.arange(0, 10, 0.01):\n",
    "    xe, ye, ze = rotate_z(x_eye, y_eye, z_eye, -t)\n",
    "    frames.append(go.Frame(layout=dict(scene_camera_eye=dict(x=xe, y=ye, z=ze))))\n",
    "fig.frames = frames\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
